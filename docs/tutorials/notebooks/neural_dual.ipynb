{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICNN Dual Solver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we explore how to learn the solution of the Kantorovich dual based on parameterizing the two dual potentials $f$ and $g$ with two input convex neural networks ({class}`~ott.solvers.nn.icnn.ICNN`) {cite}`amos:17`, a method developed by {cite}`makkuva:20`. For more insights on the approach itself, we refer the user to the original publication.\n",
    "\n",
    "Given dataloaders containing samples of the *source* and the *target* distribution, `OTT`'s {class}`~ott.solvers.nn.neuraldual.NeuralDualSolver` finds the pair of optimal potentials $f$ and $g$ to solve the corresponding dual of the optimal transport problem. Once a solution has been found, these neural {class}`~ott.problems.linear.potentials.DualPotentials` can be used to transport unseen source data samples to its target distribution (or vice-versa) or compute the corresponding distance between new source and target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install -q git+https://github.com/ott-jax/ott@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ott.geometry import pointcloud\n",
    "from ott.solvers.nn import icnn, neuraldual\n",
    "from ott.tools import sinkhorn_divergencefrom ott.solvers.nn import neuraldual, icnn, mlp\n",
    "\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define some helper functions which we use for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ot_map(neural_dual, source, target, inverse=False):\n",
    "    \"\"\"Plot data and learned optimal transport map.\"\"\"\n",
    "\n",
    "    def draw_arrows(a, b):\n",
    "        plt.arrow(\n",
    "            a[0], a[1], b[0] - a[0], b[1] - a[1], color=[0.5, 0.5, 1], alpha=0.3\n",
    "        )\n",
    "\n",
    "    grad_state_s = neural_dual.transport(source, forward=not inverse)\n",
    "\n",
    "    fig = plt.figure(facecolor=\"white\")\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if not inverse:\n",
    "        ax.scatter(\n",
    "            target[:, 0],\n",
    "            target[:, 1],\n",
    "            color=\"#A7BED3\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$target$\",\n",
    "        )\n",
    "        ax.scatter(\n",
    "            source[:, 0],\n",
    "            source[:, 1],\n",
    "            color=\"#1A254B\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$source$\",\n",
    "        )\n",
    "        ax.scatter(\n",
    "            grad_state_s[:, 0],\n",
    "            grad_state_s[:, 1],\n",
    "            color=\"#F2545B\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$\\nabla f(source)$\",\n",
    "        )\n",
    "    else:\n",
    "        ax.scatter(\n",
    "            target[:, 0],\n",
    "            target[:, 1],\n",
    "            color=\"#A7BED3\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$source$\",\n",
    "        )\n",
    "        ax.scatter(\n",
    "            source[:, 0],\n",
    "            source[:, 1],\n",
    "            color=\"#1A254B\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$target$\",\n",
    "        )\n",
    "        ax.scatter(\n",
    "            grad_state_s[:, 0],\n",
    "            grad_state_s[:, 1],\n",
    "            color=\"#F2545B\",\n",
    "            alpha=0.5,\n",
    "            label=r\"$\\nabla g(target)$\",\n",
    "        )\n",
    "\n",
    "    fig.legend(ncol=3, loc=\"upper center\")\n",
    "\n",
    "    for i in range(source.shape[0]):\n",
    "        draw_arrows(source[i, :], grad_state_s[i, :])\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_potential(neural_dual, source, target):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), facecolor=\"white\")\n",
    "    x1 = np.linspace(-6, 6)\n",
    "    x2 = np.linspace(-6, 6)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    X1flat = np.ravel(X1)\n",
    "    X2flat = np.ravel(X2)\n",
    "    X12flat = np.stack((X1flat, X2flat)).T\n",
    "    Zflat = neural_dual._f(X12flat)\n",
    "    Z = np.array(Zflat.reshape(X1.shape))\n",
    "\n",
    "    CS = ax.contourf(X1, X2, Z, cmap=\"Blues\")\n",
    "    fig.colorbar(CS, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    ax.set_title(r\"$f$\")\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer, lr, b1, b2, eps):\n",
    "    \"\"\"Returns a flax optimizer object based on `config`.\"\"\"\n",
    "\n",
    "    if optimizer == \"Adam\":\n",
    "        optimizer = optax.adam(learning_rate=lr, b1=b1, b2=b2, eps=eps)\n",
    "    elif optimizer == \"SGD\":\n",
    "        optimizer = optax.sgd(learning_rate=lr, momentum=None, nesterov=False)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {optimizer} not supported yet!\")\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_optimizer_decay(init_lr, alpha, num_train_iter, b1, b2):\n",
    "    lr_schedule = optax.cosine_decay_schedule(\n",
    "        init_value=init_lr, decay_steps=num_train_iter, alpha=alpha\n",
    "    )\n",
    "    opt = optax.adamw(learning_rate=lr_schedule, b1=b1, b2=b2)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sinkhorn_loss(x, y, epsilon=0.1):\n",
    "    \"\"\"Computes transport between (x, a) and (y, b) via Sinkhorn algorithm.\"\"\"\n",
    "    a = jnp.ones(len(x)) / len(x)\n",
    "    b = jnp.ones(len(y)) / len(y)\n",
    "\n",
    "    sdiv = sinkhorn_divergence.sinkhorn_divergence(\n",
    "        pointcloud.PointCloud, x, y, epsilon=epsilon, a=a, b=b\n",
    "    )\n",
    "    return sdiv.divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the {class}`~ott.solvers.nn.neuraldual.NeuralDualSolver` to compute the transport between toy datasets. In this tutorial, the user can choose between the datasets `simple` (data clustered in one center), `circle` (two-dimensional Gaussians arranged on a circle), `square_five` (two-dimensional Gaussians on a square with one Gaussian in the center), and `square_four` (two-dimensional Gaussians in the corners of a rectangle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ToyDataset:\n",
    "    name: str\n",
    "    batch_size: int\n",
    "    init_seed: int\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.create_sample_generators()\n",
    "\n",
    "    def create_sample_generators(self, scale=5.0, variance=0.5):\n",
    "        # given name of dataset, select centers\n",
    "        if self.name == \"simple\":\n",
    "            centers = np.array([0, 0])\n",
    "\n",
    "        elif self.name == \"circle\":\n",
    "            centers = np.array(\n",
    "                [\n",
    "                    (1, 0),\n",
    "                    (-1, 0),\n",
    "                    (0, 1),\n",
    "                    (0, -1),\n",
    "                    (1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "                    (1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "                    (-1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n",
    "                    (-1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        elif self.name == \"square_five\":\n",
    "            centers = np.array([[0, 0], [1, 1], [-1, 1], [-1, -1], [1, -1]])\n",
    "\n",
    "        elif self.name == \"square_four\":\n",
    "            centers = np.array([[1, 0], [0, 1], [-1, 0], [0, -1]])\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # create generator which randomly picks center and adds noise\n",
    "        centers = scale * centers\n",
    "        key = jax.random.PRNGKey(self.init_seed)\n",
    "        while True:\n",
    "            k1, k2, key = jax.random.split(key, 3)\n",
    "            sample_centers = jax.random.choice(k1, centers, [self.batch_size])\n",
    "            samples = sample_centers + variance**2 * jax.random.normal(\n",
    "                k2, [self.batch_size, 2]\n",
    "            )\n",
    "            yield samples\n",
    "\n",
    "\n",
    "def load_toy_data(\n",
    "    name_source: str,\n",
    "    name_target: str,\n",
    "    batch_size: int = 10000,\n",
    "    valid_batch_size: int = 1000,\n",
    "):\n",
    "    dataloaders = (\n",
    "        iter(ToyDataset(name_source, batch_size=batch_size, init_seed=0)),\n",
    "        iter(ToyDataset(name_target, batch_size=batch_size, init_seed=1)),\n",
    "        iter(ToyDataset(name_source, batch_size=valid_batch_size, init_seed=2)),\n",
    "        iter(ToyDataset(name_target, batch_size=valid_batch_size, init_seed=3)),\n",
    "    )\n",
    "    input_dim = 2\n",
    "    return dataloaders, input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Neural Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve the neural dual, we need to define our dataloaders. The only requirement is that the corresponding source and target train and validation datasets are *iterators*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dataloader_source,\n",
    "    dataloader_target,\n",
    "    dataloader_source_eval,\n",
    "    dataloader_target_eval,\n",
    "), input_dim = load_toy_data(\"square_five\", \"square_four\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the architectures parameterizing the dual potentials $f$ and $g$. These need to be parameterized by {class}`~ott.solvers.nn.icnn.ICNN`s. You can adapt the size of the ICNNs by passing a sequence containing hidden layer sizes. While ICNNs are by default containing partially positive weights, we can run the {class}`~ott.solvers.nn.neuraldual.NeuralDualSolver` using approximations to this positivity constraint (via weight clipping and a weight penalization). For this, set `positive_weights` to `True` in both the ICNN architecture and {class}`~ott.solvers.nn.neuraldual.NeuralDualSolver` configuration. For more details on how to customize the {class}`~ott.solvers.nn.icnn.ICNN` architectures, we refer you to the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models\n",
    "lrelu = partial(jax.nn.leaky_relu, negative_slope=0.2)\n",
    "neural_f = icnn.ICNN(\n",
    "    dim_data=2, dim_hidden=[128, 128], act_fn=lrelu, init_std=1.0\n",
    ")\n",
    "neural_g = mlp.PotentialGradientMLP(dim_hidden=[512, 512])\n",
    "\n",
    "# initialize optimizers\n",
    "optimizer_f = get_optimizer_decay(\n",
    "    init_lr=5e-4, alpha=1e-4, num_train_iter=50000, b1=0.5, b2=0.5\n",
    ")\n",
    "optimizer_g = optimizer_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize the {class}`~ott.solvers.nn.neuraldual.NeuralDualSolver` by passing the two {class}`~ott.solvers.nn.icnn.ICNN` models parameterizing $f$ and $g$, as well as by specifying the input dimensions of the data and the number of training iterations to execute. Once the {class}`~ott.solvers.nn.neuraldual.NeuralDualSolver` is initialized, we can obtain the neural {class}`~ott.problems.linear.potentials.DualPotentials` by passing the corresponding dataloaders to it. As here our training and validation datasets do not differ, we pass (`dataloader_source`, `dataloader_target`) for both training and validation steps. For more details on how to configure the {class}`~ott.solvers.nn.neuraldual.NeuralDualSolver`, we refer you to the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#F2545B'>Execution of the following cell might take up to 15 minutes per 5000 iterations (depending on your system and the number of training iterations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_iters = 15000\n",
    "\n",
    "# Sample an unseen batch.\n",
    "eval_data_source = next(dataloader_source_eval)\n",
    "eval_data_target = next(dataloader_target_eval)\n",
    "\n",
    "\n",
    "def training_callback(step, neural_dual):\n",
    "    # Callback function as the training progresses to visualize the couplings.\n",
    "    if step % 1000 == 0:\n",
    "        clear_output()\n",
    "        print(f\"Training iteration: {step}/{num_train_iters}\")\n",
    "\n",
    "        fig, ax = plot_ot_map(\n",
    "            neural_dual, eval_data_source, eval_data_target, inverse=False\n",
    "        )\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig, ax = plot_ot_map(\n",
    "            neural_dual, eval_data_target, eval_data_source, inverse=True\n",
    "        )\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig, ax = plot_potential(\n",
    "            neural_dual, eval_data_target, eval_data_source\n",
    "        )\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "neural_dual_solver = neuraldual.NeuralDualSolver(\n",
    "    input_dim,\n",
    "    neural_f,\n",
    "    neural_g,\n",
    "    optimizer_f,\n",
    "    optimizer_g,\n",
    "    num_train_iters=num_train_iters,\n",
    "    num_inner_iters=1,\n",
    "    amortization_loss=\"regression\",\n",
    "    parallel_updates=True,\n",
    ")\n",
    "neural_dual = neural_dual_solver(\n",
    "    dataloader_source,\n",
    "    dataloader_target,\n",
    "    dataloader_source,\n",
    "    dataloader_target,\n",
    "    callback=training_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Neural Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training has completed successfully, we can evaluate the neural {class}`~ott.problems.linear.potentials.DualPotentials` on unseen incoming data. We first sample a new batch from the source and target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the corresponding transport from source to target using the gradient of the learning potential $g$, i.e., $\\nabla g(\\text{source})$, or from target to source via the gradient of the learning potential $f$, i.e., $\\nabla f(\\text{target})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ot_map(neural_dual, eval_data_source, eval_data_target, inverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ot_map(neural_dual, eval_data_target, eval_data_source, inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further test, how close the predicted samples are to the sampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for potential $g$, transporting source to target samples. Ideally the resulting {class}`~ott.solvers.linear.sinkhorn.Sinkhorn` distance is close to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target = neural_dual.transport(eval_data_source)\n",
    "print(\n",
    "    f\"Sinkhorn distance between predictions and data samples: {sinkhorn_loss(pred_target, eval_data_target)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for potential $f$, transporting target to source samples. Again, the resulting {class}`~ott.solvers.linear.sinkhorn.Sinkhorn` distance needs to be close to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_source = neural_dual.transport(eval_data_target, forward=False)\n",
    "print(\n",
    "    f\"Sinkhorn distance between predictions and data samples: {sinkhorn_loss(pred_source, eval_data_source)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides computing the transport and mapping source to target samples or vice versa, we can also compute the overall distance between new source and target samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_dual_dist = neural_dual.distance(eval_data_source, eval_data_target)\n",
    "print(\n",
    "    f\"Neural dual distance between source and target data: {neural_dual_dist}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which compares to the primal {class}`~ott.solvers.linear.sinkhorn.Sinkhorn` distance in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinkhorn_dist = sinkhorn_loss(eval_data_source, eval_data_target)\n",
    "print(f\"Sinkhorn distance between source and target data: {sinkhorn_dist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
