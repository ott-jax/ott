{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Dual Solver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to use `OTT` to compute the Wasserstein-4 optimal transport map between continuous measures in Euclidean space that are accessible via sampling.\n",
    "{class}`~ott.solvers.nn.neuraldual.W2NeuralDual` solves this\n",
    "problem by optimizing parameterized Kantorovich dual potential functions\n",
    "and returning a  {class}`~ott.problems.linear.potentials.DualPotentials`\n",
    "object that can be used to transport unseen source data samples to its target distribution (or vice-versa) or compute the corresponding distance between new source and target distribution.\n",
    "\n",
    "The dual potentials can be specified as non-convex neural networks\n",
    "({class}`~ott.solvers.nn.models.MLP`) or an\n",
    "input-convex neural network ({class}`~ott.solvers.nn.models.ICNN`) {cite}`amos:17`.\n",
    "{class}`~ott.solvers.nn.neuraldual.W2NeuralDual` implements the\n",
    "method developed by {cite}`makkuva:20`\n",
    "along with the improvements and fine-tuning of the\n",
    "conjugate computation from {cite}`amos:23`.\n",
    "For more insights on the approach itself, we refer the user\n",
    "to the original sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !pip install -q git+https://github.com/ott-jax/ott@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ott.geometry import pointcloud\n",
    "from ott.problems.linear import potentials\n",
    "from ott.problems.nn import dataset\n",
    "from ott.solvers.nn import models, neuraldual\n",
    "from ott.tools import plot, sinkhorn_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the {class}`~ott.solvers.nn.neuraldual.W2NeuralDual` to compute the transport between toy datasets. In this tutorial, the user can choose between the datasets `simple` (data clustered in one center), `circle` (two-dimensional Gaussians arranged on a circle), `square_five` (two-dimensional Gaussians on a square with one Gaussian in the center), and `square_four` (two-dimensional Gaussians in the corners of a rectangle).\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Neural Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve the neural dual, we need to define our dataloaders. The only requirement is that the corresponding source and target train and validation datasets are *iterators* that provide samples of batches from the source and target measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders, input_dim = dataset.create(\n",
    "    name_source=\"gaussian_simple\", name_target=\"gaussian_circle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(eval_data_source, eval_data_target):\n",
    "    fig, axs = plt.subplots(\n",
    "        1, 2, figsize=(8, 4), gridspec_kw={\"wspace\": 0, \"hspace\": 0}\n",
    "    )\n",
    "    axs[0].scatter(\n",
    "        eval_data_source[:, 0],\n",
    "        eval_data_source[:, 1],\n",
    "        color=\"#A7BED3\",\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[0].set_title(\"Source measure sample\")\n",
    "    axs[1].scatter(\n",
    "        eval_data_target[:, 0],\n",
    "        eval_data_target[:, 1],\n",
    "        color=\"#1A254B\",\n",
    "        s=10,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[1].set_title(\"Target measure samples\")\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlim(-6, 6)\n",
    "        ax.set_ylim(-6, 6)\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Sample a batch for evaluation and plot it\n",
    "eval_data_source = next(dataloaders.validloader_source)\n",
    "eval_data_target = next(dataloaders.validloader_target)\n",
    "\n",
    "plot_samples(eval_data_source, eval_data_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the architectures parameterizing the dual potentials $f$ and $g$. We first parameterize $f$ with an {class}`~ott.solvers.nn.models.ICNN` and $\\nabla g$ as a non-convex {class}`~ott.solvers.nn.models.MLP`. You can adapt the size of the ICNNs by passing a sequence containing hidden layer sizes. While ICNNs are by default containing partially positive weights, we can run the {class}`~ott.solvers.nn.neuraldual.W2NeuralDual` using approximations to this positivity constraint (via weight clipping and a weight penalization). For this, set `positive_weights` to `True` in both the ICNN architecture and {class}`~ott.solvers.nn.neuraldual.W2NeuralDual` configuration. For more details on how to customize the {class}`~ott.solvers.nn.models.ICNN` architectures, we refer you to the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models and optimizers\n",
    "num_train_iters = 5001\n",
    "\n",
    "neural_f = models.ICNN(dim_data=2, dim_hidden=[128, 128])\n",
    "neural_g = models.MLP(\n",
    "    dim_hidden=[128, 128],\n",
    "    is_potential=False,  # returns the gradient of the potential.\n",
    ")\n",
    "\n",
    "lr_schedule = optax.cosine_decay_schedule(\n",
    "    init_value=5e-4, decay_steps=num_train_iters, alpha=1e-2\n",
    ")\n",
    "optimizer_f = optax.adam(learning_rate=lr_schedule, b1=0.5, b2=0.5)\n",
    "optimizer_g = optax.adam(learning_rate=lr_schedule, b1=0.9, b2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize the {class}`~ott.solvers.nn.neuraldual.W2NeuralDual` by passing the two {class}`~ott.solvers.nn.models.ICNN` models parameterizing $f$ and $g$, as well as by specifying the input dimensions of the data and the number of training iterations to execute. Once the {class}`~ott.solvers.nn.neuraldual.W2NeuralDual` is initialized, we can obtain the neural {class}`~ott.problems.linear.potentials.DualPotentials` by passing the corresponding dataloaders to it. As here our training and validation datasets do not differ, we pass (`dataloader_source`, `dataloader_target`) for both training and validation steps. For more details on how to configure the {class}`~ott.solvers.nn.neuraldual.W2NeuralDual`, we refer you to the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#F2545B'>Execution of the following cell will probably take a few minutes, depending on your system and the number of training iterations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_callback(step, learned_potentials):\n",
    "    # Callback function as the training progresses to visualize the couplings.\n",
    "    if step % 1000 == 0:\n",
    "        clear_output()\n",
    "        print(f\"Training iteration: {step}/{num_train_iters}\")\n",
    "\n",
    "        fig, ax = plot.plot_ot_map(\n",
    "            learned_potentials, eval_data_source, eval_data_target, forward=True\n",
    "        )\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig, ax = plot.plot_ot_map(\n",
    "            learned_potentials,\n",
    "            eval_data_source,\n",
    "            eval_data_target,\n",
    "            forward=False,\n",
    "        )\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig, ax = plot.plot_potential(learned_potentials)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "neural_dual_solver = neuraldual.W2NeuralDual(\n",
    "    input_dim,\n",
    "    neural_f,\n",
    "    neural_g,\n",
    "    optimizer_f,\n",
    "    optimizer_g,\n",
    "    num_train_iters=num_train_iters,\n",
    ")\n",
    "learned_potentials = neural_dual_solver(\n",
    "    *dataloaders,\n",
    "    callback=training_callback,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the solver, `learned_potentials`, is an instance of\n",
    "{class}`~ott.problems.linear.potentials.DualPotentials`.\n",
    "This  gives us access to the learned potentials and provides functions to compute and plot the forward and\n",
    "inverse OT maps between the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot.plot_potential(learned_potentials, forward=True)\n",
    "plot.plot_potential(learned_potentials, forward=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Neural Dual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training has completed successfully, we can evaluate the neural {class}`~ott.problems.linear.potentials.DualPotentials` on unseen incoming data. We first sample a new batch from the source and target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the corresponding transport from source to target using the gradient of the learning potential $g$, i.e., $\\nabla g(\\text{source})$, or from target to source via the gradient of the learning potential $f$, i.e., $\\nabla f(\\text{target})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_ot_map(\n",
    "    learned_potentials, eval_data_source, eval_data_target, forward=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_ot_map(\n",
    "    learned_potentials, eval_data_source, eval_data_target, forward=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further test, how close the predicted samples are to the sampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for potential $g$, transporting source to target samples. Ideally the resulting {class}`~ott.solvers.linear.sinkhorn.Sinkhorn` distance is close to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sinkhorn_loss(x, y, epsilon=0.1):\n",
    "    \"\"\"Computes transport between (x, a) and (y, b) via Sinkhorn algorithm.\"\"\"\n",
    "    a = jnp.ones(len(x)) / len(x)\n",
    "    b = jnp.ones(len(y)) / len(y)\n",
    "\n",
    "    sdiv = sinkhorn_divergence.sinkhorn_divergence(\n",
    "        pointcloud.PointCloud, x, y, epsilon=epsilon, a=a, b=b\n",
    "    )\n",
    "    return sdiv.divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_target = learned_potentials.transport(eval_data_source)\n",
    "print(\n",
    "    f\"Sinkhorn distance between source predictions and data samples: {sinkhorn_loss(pred_target, eval_data_target):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for potential $f$, transporting target to source samples. Again, the resulting {class}`~ott.solvers.linear.sinkhorn.Sinkhorn` distance needs to be close to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_source = learned_potentials.transport(eval_data_target, forward=False)\n",
    "print(\n",
    "    f\"Sinkhorn distance between source predictions and data samples: {sinkhorn_loss(pred_source, eval_data_source):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides computing the transport and mapping source to target samples or vice versa, we can also compute the overall distance between new source and target samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_dual_dist = learned_potentials.distance(\n",
    "    eval_data_source, eval_data_target\n",
    ")\n",
    "print(\n",
    "    f\"Neural dual distance between source and target data: {neural_dual_dist:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which compares to the primal {class}`~ott.solvers.linear.sinkhorn.Sinkhorn` distance in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinkhorn_dist = sinkhorn_loss(eval_data_source, eval_data_target)\n",
    "print(f\"Sinkhorn distance between source and target data: {sinkhorn_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving a harder problem\n",
    "\n",
    "We next set up a harder OT problem to transport from a mixture\n",
    "of five Gaussians to a mixture of four Gaussians and solve\n",
    "it by using the non-convex {class}`~ott.solvers.nn.models.MLP`\n",
    "potentials to model $f$ and $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders, input_dim = dataset.create(\n",
    "    name_source=\"gaussian_square_five\", name_target=\"gaussian_square_four\"\n",
    ")\n",
    "\n",
    "eval_data_source = next(dataloaders.validloader_source)\n",
    "eval_data_target = next(dataloader.validloader_target)\n",
    "plot_samples(eval_data_source, eval_data_target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_train_iters = 20001\n",
    "\n",
    "neural_f = models.MLP(dim_hidden=[128, 128])\n",
    "neural_g = models.MLP(dim_hidden=[128, 128])\n",
    "\n",
    "lr_schedule = optax.cosine_decay_schedule(\n",
    "    init_value=5e-4, decay_steps=num_train_iters, alpha=1e-2\n",
    ")\n",
    "optimizer_f = optax.adamw(learning_rate=lr_schedule)\n",
    "optimizer_g = optimizer_f\n",
    "\n",
    "neural_dual_solver = neuraldual.W2NeuralDual(\n",
    "    input_dim,\n",
    "    neural_f,\n",
    "    neural_g,\n",
    "    optimizer_f,\n",
    "    optimizer_g,\n",
    "    num_train_iters=num_train_iters,\n",
    ")\n",
    "learned_potentials = neural_dual_solver(\n",
    "    *dataloaders,\n",
    "    callback=training_callback,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the same visualizations and Wasserstein-2 distance estimations as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot.plot_ot_map(\n",
    "    learned_potentials, eval_data_source, eval_data_target, forward=True\n",
    ")\n",
    "plot.plot_ot_map(\n",
    "    learned_potentials, eval_data_source, eval_data_target, forward=False\n",
    ")\n",
    "\n",
    "pred_target = learned_potentials.transport(eval_data_source)\n",
    "print(\n",
    "    f\"Sinkhorn distance between target predictions and data samples: {sinkhorn_loss(pred_target, eval_data_target):.2f}\"\n",
    ")\n",
    "\n",
    "\n",
    "pred_source = learned_potentials.transport(eval_data_target, forward=False)\n",
    "print(\n",
    "    f\"Sinkhorn distance between source predictions and data samples: {sinkhorn_loss(pred_source, eval_data_source):.2f}\"\n",
    ")\n",
    "\n",
    "neural_dual_dist = learned_potentials.distance(\n",
    "    eval_data_source, eval_data_target\n",
    ")\n",
    "print(\n",
    "    f\"Neural dual distance between source and target data: {neural_dual_dist:.2f}\"\n",
    ")\n",
    "\n",
    "sinkhorn_dist = sinkhorn_loss(eval_data_source, eval_data_target)\n",
    "print(f\"Sinkhorn distance between source and target data: {sinkhorn_dist:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
