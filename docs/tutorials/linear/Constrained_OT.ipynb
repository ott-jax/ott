{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5580f69d",
   "metadata": {},
   "source": [
    "# Constrained Optimal Transport\n",
    "\n",
    "This notebook provides a tutorial on using the {class}`OTT's <ott.solvers.linear.sinkhorn.Sinkhorn>` to solve constrained optimal transport (COT) problems. The tutorial heavily relies on the paper {cite}`tang:24` and the ott-jax package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02723f8b",
   "metadata": {},
   "source": [
    "## Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaaedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# from jax.scipy.sparse import csr_matrix\n",
    "from jax.experimental import sparse as jsparse\n",
    "from jax.scipy.sparse.linalg import cg\n",
    "\n",
    "from ott.geometry import pointcloud\n",
    "from ott.geometry.geometry import Geometry\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68909bb7",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewtonWrapper:\n",
    "    \"\"\"Wrapper for the Newton solver.\n",
    "    This wrapper is used to provide a consistent interface for the\n",
    "    Newton solver, which is used to solve the linear problem.\n",
    "\n",
    "    Attributes:\n",
    "        obj: The objective function value.\n",
    "        grad: The gradient of the objective function.\n",
    "        hess: The Hessian of the objective function.\n",
    "    \"\"\"\n",
    "\n",
    "    obj: Any\n",
    "    grad: Any\n",
    "    hess: Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6d165",
   "metadata": {},
   "source": [
    "## Constrained Linear Problem (CLP)\n",
    "\n",
    "Given $a, b \\in \\mathbb{R}^n$, a cost matrix $C \\in \\mathbb{R}^{n \\times n}$, and matrices $D_1, \\dots, D_K, D_{K+1}, \\dots, D_{K+L} \\in \\mathbb{R}^{n \\times n}$, the corresponding constrained optimal transport problem (COT) is formulated as:\n",
    "\n",
    "$$\n",
    "\\min_{P \\in \\mathcal{U}(a, b)} \\langle C, P \\rangle \\quad \\text{subject to} \\quad \n",
    "\\begin{array}{cc}\n",
    "\\forall k = 1, \\dots, K & \\langle D_k, P \\rangle \\geq 0 \\\\\n",
    "\\forall l = 1, \\dots, L & \\langle D_{K+l}, P \\rangle = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathcal{U}(a, b) $ is the set of transport plans $ P $ such that $ P1 = a $ and $ P^T1 = b $ (i.e., the marginals are fixed),\n",
    "- $ \\langle C, P \\rangle $ represents the inner product between the cost matrix $C$ and the transport plan $P$, which is computed as $ \\sum_{i,j} C_{ij} P_{ij} $,\n",
    "- $ D_k $ are matrices representing **non-negativity** constraints on the transport plan (i.e., $ \\langle D_k, P \\rangle \\geq 0 $ for $ k = 1, \\dots, K $),\n",
    "- $ D_{K+l} $ are matrices representing **equality** constraints on the transport plan (i.e., $ \\langle D_{K+l}, P \\rangle = 0 $ for $ l = 1, \\dots, L $).\n",
    "\n",
    "### Core Attributes of Constrained Linear Problem (CLP)\n",
    "\n",
    "The core attributes of the constrained linear problem are implemented in the `ConstrainedLinearProblem` class. The class utilizes the `Geometry` class to define the cost matrix and the underlying geometry of the problem.\n",
    "\n",
    "#### Key Attributes:\n",
    "\n",
    "- **`a` (jnp.array)**: The first marginal distribution $a $ with shape $ (n,) $.\n",
    "- **`b` (jnp.array)**: The second marginal distribution $ b $ with shape $ (n,) $.\n",
    "- **`geom` (Geometry)**: An object that encapsulates the geometry (e.g., distance or structure) of the problem. This can include additional information, such as the cost matrix $ C $.\n",
    "- **`constraints` (jnp.array, optional)**: The constraint matrices $ D_1, \\dots, D_K, D_{K+1}, \\dots, D_{K+L} $, typically of shape $ (n, n, K+L) $, specifying linear constraints on the transport matrix.\n",
    "\n",
    "This class provides methods to:\n",
    "- Round an approximate transport matrix to ensure it lies in the feasible set $ \\mathcal{U}(a, b) $.\n",
    "- Compute the cost of an approximate transport plan, taking into account the cost matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedLinearProblem:\n",
    "    \"\"\"\n",
    "    This class implements a constrained linear problem with a cost matrix,\n",
    "    constraints, and a regularization parameter epsilon. It provides methods\n",
    "    to round an approximate transport matrix to ensure it lies in the set U(a,b)\n",
    "    and to compute the cost of an approximate transport plan.\n",
    "\n",
    "    Attributes:\n",
    "        a (jnp.array): distribution a of shape (n,)\n",
    "        b (jnp.array): distribution b of shape (n,)\n",
    "        epsilon (float): regularization parameter\n",
    "        cosh_matrix (jnp.array): cost matrix of shape (n,n)\n",
    "        constraints (jnp.array): constraints for the transport problem of shape (n,n,K+L)\n",
    "\n",
    "    Methods:\n",
    "        round(F: jnp.array) -> jnp.array:\n",
    "            Round an approximate transport matrix F to ensure it lies in\n",
    "            the set U(a,b)={F: F1=a, F^T1=b, F>=0}.\n",
    "        cost(P: jnp.array) -> jnp.array:\n",
    "            Compute the cost of an approximate transport plan P.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        geom: Geometry,\n",
    "        a: jnp.array,\n",
    "        b: jnp.array,\n",
    "        constraints: jnp.array = None,\n",
    "    ):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.geom = geom\n",
    "        self.constraints = constraints\n",
    "\n",
    "    def round(self, F: jnp.array) -> jnp.array:\n",
    "        \"\"\"Round an approximate transport matrix F to ensure it lies in\n",
    "        the set U(a,b)={F: F1=a, F^T1=b, F>=0}.\n",
    "\n",
    "        Args:\n",
    "            F (jnp.array): approximate transport matrix of shape (n,n)\n",
    "\n",
    "        Returns:\n",
    "            jnp.array: rounded transport matrix lying in U(a,b)\n",
    "        \"\"\"\n",
    "        X = jnp.diag(jnp.minimum(self.a / jnp.sum(F, axis=1), 1))\n",
    "        F = X @ F\n",
    "        Y = jnp.diag(jnp.minimum(self.b / jnp.sum(F, axis=0), 1))\n",
    "        F = F @ Y\n",
    "        err_a, err_b = self.a - jnp.sum(F, axis=1), self.b - jnp.sum(F, axis=0)\n",
    "        return F + jnp.outer(err_a, err_b) / jnp.sum(err_a)\n",
    "\n",
    "    def cost(self, P: jnp.array) -> jnp.array:\n",
    "        \"\"\"Compute the cost of an approximate transport plan P.\n",
    "\n",
    "        Args:\n",
    "            P (jnp.array): approximate transport plan of shape (n,n)\n",
    "\n",
    "        Returns:\n",
    "            jnp.array: cost of the transport plan\n",
    "        \"\"\"\n",
    "        return jnp.sum(self.geom.cost_matrix * round(P, self.a, self.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5e6641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = jnp.array([0.2, 0.5, 0.3])\n",
    "b = jnp.array([0.4, 0.1, 0.5])\n",
    "\n",
    "F = jnp.ones((len(a), len(b)))  # or use random values if you prefer\n",
    "\n",
    "jnp.sum(a).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db0e75",
   "metadata": {},
   "source": [
    "## Entropic Relaxation\n",
    "\n",
    "Motivated by the results in {cite}`tang:24`, we solve an entropic relaxation of the Constrained Optimal Transport (COT) problem. This relaxation provides an exponentially close solution to the original COT problem. The entropic relaxation is formulated as follows:\n",
    "\n",
    "$$\n",
    "\\min_{P \\in \\mathcal{U}(a, b), \\, s \\in \\mathbb{R}_+^K}  \\langle C, P \\rangle + \\varepsilon H(P, s_1, \\dots, s_K)  \\quad \\text{subject to} \\quad \n",
    "\\begin{array}{cc}\n",
    "\\forall k = 1, \\dots, K & \\langle D_k, P \\rangle = s_k \\\\\n",
    "\\forall l = 1, \\dots, L & \\langle D_{K+l}, P \\rangle = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here, $ H(P, s_1, \\dots, s_K) $ represents the entropy regularization term for a transport plan $P$ and scalars $s_1, \\dots, s_K$, which is computed as $H(P, s_1, \\dots, s_K) = \\sum_{1 \\leq i,j \\leq n} P_{ij} \\log P_{ij} + \\sum_{k=1}^{K} s_k \\log s_k$.\n",
    "\n",
    "### Primal-Dual Formulation\n",
    "\n",
    "The corresponding primal-dual formulation of the entropic relaxation is:\n",
    "\n",
    "$$\n",
    "\\max_{u, v \\in \\mathbb{R}^n; \\, h \\in \\mathbb{R}^{K+L}} \\min_{P, s} \\mathcal{L}_\\varepsilon (u, v, h; P, s)\n",
    "$$\n",
    "\n",
    "where the Lagrangian $ \\mathcal{L}_\\varepsilon  $ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_\\varepsilon (u, v, h; P, s) := \\varepsilon \\langle P, \\log P \\rangle + \\langle C, P \\rangle - \\langle u, P\\mathbf{1} - a \\rangle - \\langle v, P^T\\mathbf{1} - b \\rangle + \\varepsilon \\sum_{k=1}^K s_k \\log s_k + \\sum_{k=1}^K h_k s_k + \\sum_{m=1}^{K+L} h_m \\langle D_m, P \\rangle\n",
    "$$\n",
    "\n",
    "We define the Lyapunov function as:\n",
    "\n",
    "$$\n",
    "f(u, v, h) = \\min_{P, s} \\mathcal{L}_\\varepsilon (u, v, h; P, s).\n",
    "$$\n",
    "\n",
    "The corresponding intermediate transport plan $ P_\\varepsilon(u, v, h) $ is given by:\n",
    "\n",
    "$$\n",
    "P_\\varepsilon (u,v,h) = \\exp \\left( \\frac{1}{\\varepsilon} \\left( -C + \\sum_{m=1}^{K+L} h_m D_m + u \\mathbf{1}^T + \\mathbf{1} v^T \\right) - 1 \\right).\n",
    "$$\n",
    "\n",
    "The Lyapunov function $ f(u, v, h) $ has the following explicit formulation:\n",
    "\n",
    "$$\n",
    "f(u, v, t) = -\\varepsilon \\sum_{1 \\leq i, j \\leq n} \\exp \\left( \\frac{1}{\\varepsilon} \\left( -C_{ij} + \\sum_{m=1}^{K+L} h_m (D_m)_{ij} + u_i + v_j \\right) - 1 \\right)\n",
    "+ \\sum_{i=1}^n u_i a_i + \\sum_{j=1}^n v_j b_j - \\varepsilon \\sum_{k=1}^K \\exp \\left( - \\frac{1}{\\varepsilon} h_k - 1 \\right)\n",
    "$$\n",
    "\n",
    "By the minimax theorem, solving the entropic relaxation problem is equivalent to maximizing $f$. After maximizing for $(u, v, h)$, the corresponding optimal transport plan $ P_\\varepsilon^* $ is a smoothed approximation of the original transport matrix $P^*$. As $\\varepsilon \\to 0$, the solution $ P_\\varepsilon^*$ converges to the optimal solution of the original CLP problem.\n",
    "\n",
    "### Constrained Sinkhorn Solver\n",
    "\n",
    "The `ConstrainedSinkhorn` class is an implementation of the Sinkhorn algorithm tailored for regularized optimal transport with additional constraints. It is designed to solve constrained linear problem (CLP) problems. \n",
    "\n",
    "The algorithm iteratively computes a transport plan that satisfies the marginal constraints (given by `a` and `b`) while minimizing the transportation cost subject to the constraints. The solver utilizes both Sinkhorn scaling steps and Newton steps to ensure convergence, with additional flexibility provided by a dual-optimization approach.\n",
    "\n",
    "The state of the solver is encapsulated in the `ConstrainedSinkhornState` dataclass. This state holds the current values of the dual variables $ u $, $ v $, and $ h $, as well as information about the solver's convergence and error.\n",
    "\n",
    "#### Fields:\n",
    "- `u`: The dual variable corresponding to the row scaling (size $ n $).\n",
    "- `v`: The dual variable corresponding to the column scaling (size $ n $).\n",
    "- `h`: The dual variable associated with the constraints (size $ K + L$).\n",
    "- `error`: (Optional) The current error of the solution, used for convergence checking.\n",
    "- `converged`: Boolean flag indicating whether the solver has converged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConstrainedSinkhornState:\n",
    "    \"\"\"State of the Constrained Sinkhorn solver.\"\"\"\n",
    "\n",
    "    u: jnp.ndarray\n",
    "    v: jnp.ndarray\n",
    "    h: jnp.ndarray\n",
    "    error: Optional[float] = None\n",
    "    converged: bool = False\n",
    "\n",
    "\n",
    "class ConstrainedSinkhorn:\n",
    "    \"\"\"\n",
    "    Constrained Sinkhorn solver for regularized optimal transport with additional constraints.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_sns: bool = False,\n",
    "        N1: int = 20,\n",
    "        N2: int = 80,\n",
    "        threshold: float = 1e-3,\n",
    "        min_iterations: int = 10,\n",
    "        max_iterations: int = 2000,\n",
    "        inner_iterations: int = 1,\n",
    "        implicih_differentiation: bool = True,\n",
    "    ):\n",
    "        self.use_sns = use_sns\n",
    "        self.N1 = N1\n",
    "        self.N2 = N2\n",
    "        self.threshold = threshold\n",
    "        self.min_iterations = min_iterations\n",
    "        self.max_iterations = max_iterations\n",
    "        self.inner_iterations = inner_iterations\n",
    "        self.implicih_differentiation = implicih_differentiation\n",
    "\n",
    "    def init(self, cot_lp) -> ConstrainedSinkhornState:\n",
    "        \"\"\"Initialize dual variables.\"\"\"\n",
    "        n = cot_lp.a.shape[0]\n",
    "        u = jnp.zeros(n)\n",
    "        v = jnp.zeros(n)\n",
    "        h = jnp.zeros(cot_lp.constraints[\"matrices\"].shape[2])\n",
    "        return ConstrainedSinkhornState(u=u, v=v, h=h)\n",
    "\n",
    "    def _apply_transport(\n",
    "        self, cot_lp, u: jnp.ndarray, v: jnp.ndarray, h: jnp.ndarray\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Compute intermediate transport plan.\"\"\"\n",
    "        Ds = cot_lp.constraints[\"matrices\"]\n",
    "        K = cot_lp.constraints[\"K\"]\n",
    "        modulation = jnp.tensordot(Ds[:, :, :K], h[:K], axes=([2], [0]))\n",
    "        base_transport = cot_lp.geom.transporh_from_potentials(u, v)\n",
    "        return (\n",
    "            base_transport\n",
    "            * jnp.exp(modulation / cot_lp.geom.epsilon)\n",
    "            / jnp.exp(1)\n",
    "        )\n",
    "\n",
    "    def _scaling_step(\n",
    "        self, cot_lp, u: jnp.ndarray, v: jnp.ndarray, h: jnp.ndarray, axis: int\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Perform a scaling step (row or column).\"\"\"\n",
    "        transp = self._apply_transport(cot_lp, u, v, h)\n",
    "        if axis == 1:\n",
    "            marg = jnp.sum(transp, axis=1)\n",
    "            target = cot_lp.a\n",
    "        else:\n",
    "            marg = jnp.sum(transp, axis=0)\n",
    "            target = cot_lp.b\n",
    "        return cot_lp.geom.epsilon * (jnp.log(target) - jnp.log(marg))\n",
    "\n",
    "    def _lyapunov(\n",
    "        self,\n",
    "        cot_lp,\n",
    "        u: jnp.ndarray,\n",
    "        v: jnp.ndarray,\n",
    "        h: jnp.ndarray,\n",
    "        sns: bool = False,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Compute Lyapunov function.\"\"\"\n",
    "        transp = self._apply_transport(cot_lp, u, v, h)\n",
    "        epsilon = cot_lp.geom.epsilon\n",
    "        K = cot_lp.constraints[\"K\"]\n",
    "        lyapunov = (\n",
    "            -epsilon * jnp.sum(transp)\n",
    "            + jnp.sum(u * cot_lp.a)\n",
    "            + jnp.sum(v * cot_lp.b)\n",
    "            - epsilon * jnp.sum(jnp.exp(-(1 / epsilon) * h[:K] - 1))\n",
    "        )\n",
    "        if sns:\n",
    "            lyapunov -= 0.5 * jnp.sum(u - v) ** 2\n",
    "        return lyapunov\n",
    "\n",
    "    def _projech_v_perp(self, z: jnp.ndarray, n: int) -> jnp.ndarray:\n",
    "        \"\"\"Project onto v⊥, v = (1_n, -1_n, 0)\"\"\"\n",
    "        v = jnp.concatenate(\n",
    "            [jnp.ones(n), -jnp.ones(n), jnp.zeros(z.shape[0] - 2 * n)]\n",
    "        )\n",
    "        return z - jnp.dot(z, v) / jnp.dot(v, v) * v\n",
    "\n",
    "    def _sparsify(P: jnp.ndarray, rho: jnp.array) -> jnp.ndarray:\n",
    "        \"\"\"Sparsify the transport plan.\"\"\"\n",
    "        P = jnp.where(P > rho, P, 0.0)\n",
    "        return P\n",
    "\n",
    "    def _approximate_hessian(\n",
    "        self,\n",
    "        lyap: function,\n",
    "        transp: jnp.ndarray,\n",
    "        u: jnp.ndarray,\n",
    "        v: jnp.ndarray,\n",
    "        h: jnp.ndarray,\n",
    "        rho: float = 0.0,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Compute approximate Hessian for the SNS Loop.\"\"\"\n",
    "        diag_px = jnp.diag(transp.sum(axis=1))\n",
    "        diag_py = jnp.diag(transp.sum(axis=0))\n",
    "\n",
    "        P_sparse = self._sparsify(transp, rho)\n",
    "\n",
    "        hess_uh = jax.hessian(lyap, (0, 2))(u, v, h)  # ∇²_{u,a}\n",
    "        hess_vh = jax.hessian(lyap, (1, 2))(u, v, h)  # ∇²_{v,a}\n",
    "        hess_hh = jax.hessian(lyap, (2, 2))(u, v, h)  # ∇²_{a,a}\n",
    "\n",
    "        row1 = jnp.hstack([diag_px, P_sparse, hess_uh])\n",
    "        row2 = jnp.hstack([P_sparse.T, diag_py, hess_vh])\n",
    "        row3 = jnp.hstack([hess_uh.T, hess_vh.T, hess_hh])\n",
    "\n",
    "        return jnp.vstack([row1, row2, row3])\n",
    "\n",
    "    def _regularize_hessian(self, hess: jnp.ndarray, n: int) -> jnp.ndarray:\n",
    "        v = jnp.concatenate(\n",
    "            [jnp.ones(n), -jnp.ones(n), jnp.zeros(hess.shape[0] - 2 * n)]\n",
    "        )\n",
    "        return hess - v @ v.T\n",
    "\n",
    "    def _tosparse(mat: jnp.ndarray) -> jsparse.BCOO:\n",
    "        \"\"\"Convert a dense matrix to sparse format.\"\"\"\n",
    "        return jsparse.BCOO.fromdense(mat)\n",
    "\n",
    "    def _todense(mat: jsparse.BCOO) -> jnp.ndarray:\n",
    "        \"\"\"Convert a sparse matrix to dense format.\"\"\"\n",
    "        return mat.todense()\n",
    "\n",
    "    def _solve_linear_system(\n",
    "        self, A: jnp.ndarray, y: jnp.ndarray, **kwargs\n",
    "    ) -> tuple[jnp.ndarray, int]:\n",
    "        \"\"\"Solve the linear system Ax = y using conjugate gradient.\"\"\"\n",
    "        A = self._tosparse(A)\n",
    "        x, info = cg(A, y, **kwargs)\n",
    "        return x, info\n",
    "\n",
    "    def _newton_step(\n",
    "        self,\n",
    "        cot_lp,\n",
    "        u: jnp.ndarray,\n",
    "        v: jnp.ndarray,\n",
    "        convergence_tol: float = 1e-6,\n",
    "        max_iter: int = 20,\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Compute Newton step for updating h variables.\"\"\"\n",
    "\n",
    "        def objective(w: jnp.ndarray) -> jnp.ndarray:\n",
    "            return self._lyapunov(\n",
    "                cot_lp, u + w[:1] * jnp.ones_like(u), v, w[1:]\n",
    "            )\n",
    "\n",
    "        grad_fn = jax.grad(objective)\n",
    "        hess_fn = jax.hessian(objective)\n",
    "        newton_wrap = NewtonWrapper(obj=objective, grad=grad_fn, hess=hess_fn)\n",
    "\n",
    "        w = jnp.ones(cot_lp.constraints[\"matrices\"].shape[2] + 1)\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            grad = grad_fn(w)\n",
    "            hess = hess_fn(w)\n",
    "            delta = -jnp.linalg.solve(hess, grad)\n",
    "            alpha = self._backtracking_line_search(newton_wrap, w, delta)\n",
    "            w = w + alpha * delta\n",
    "\n",
    "            if jnp.linalg.norm(delta) < convergence_tol:\n",
    "                break\n",
    "\n",
    "        if jnp.linalg.norm(delta) >= convergence_tol:\n",
    "            raise ValueError(\"Newton method did not converge.\")\n",
    "\n",
    "        return w\n",
    "\n",
    "    def _backtracking_line_search(\n",
    "        self,\n",
    "        newton_wrap,\n",
    "        t: jnp.ndarray,\n",
    "        delta: jnp.ndarray,\n",
    "        alpha_start: float = 1.0,\n",
    "        rho: float = 0.5,\n",
    "        c: float = 1e-4,\n",
    "        max_line_search_iter: int = 20,\n",
    "    ) -> float:\n",
    "        \"\"\"Backtracking line search for Newton step.\"\"\"\n",
    "        alpha = alpha_start\n",
    "        grad = newton_wrap.grad(t)\n",
    "        slope = jnp.dot(grad, delta)\n",
    "\n",
    "        for _ in range(max_line_search_iter):\n",
    "            if (\n",
    "                newton_wrap.obj(t + alpha * delta)\n",
    "                <= newton_wrap.obj(t) + c * alpha * slope\n",
    "            ):\n",
    "                break\n",
    "            alpha *= rho\n",
    "\n",
    "        if alpha < 1e-10:\n",
    "            raise ValueError(\"Line search failed.\")\n",
    "        return alpha\n",
    "\n",
    "    def _one_iteration_sinkhorn(\n",
    "        self, cot_lp, state: ConstrainedSinkhornState, **kwargs\n",
    "    ) -> ConstrainedSinkhornState:\n",
    "        \"\"\"Perform one iteration of constrained Sinkhorn algorithm.\n",
    "\n",
    "        Args:\n",
    "            cot_lp: Constrained linear problem.\n",
    "            state: Current state of the solver.\n",
    "            iteration: Current iteration number.\n",
    "\n",
    "        Returns:\n",
    "            ConstrainedSinkhornState: Updated state of the solver.\n",
    "        \"\"\"\n",
    "\n",
    "        u, v, h = state.u, state.v, state.h\n",
    "\n",
    "        # Scale rows\n",
    "        u += self._scaling_step(cot_lp, u, v, h, axis=1)\n",
    "\n",
    "        # Scale columns\n",
    "        v += self._scaling_step(cot_lp, u, v, h, axis=0)\n",
    "\n",
    "        # Newton step for constraint correction\n",
    "        w = self._newton_step(cot_lp, u, v, **kwargs)\n",
    "        u += w[0] * jnp.ones_like(u)\n",
    "        h += w[1:]\n",
    "\n",
    "        return ConstrainedSinkhornState(u=u, v=v, h=h)\n",
    "\n",
    "    def _one_iteration_newton(\n",
    "        self,\n",
    "        cot_lp,\n",
    "        state: ConstrainedSinkhornState,\n",
    "        rho: float = 1e-6,\n",
    "        **kwargs,\n",
    "    ) -> ConstrainedSinkhornState:\n",
    "        \"\"\"Perform one iteration of the Newton method.\n",
    "\n",
    "        Args:\n",
    "            cot_lp: Constrained linear problem.\n",
    "            state: Current state of the solver.\n",
    "            iteration: Current iteration number.\n",
    "\n",
    "        Returns:\n",
    "            ConstrainedSinkhornState: Updated state of the solver.\n",
    "        \"\"\"\n",
    "\n",
    "        u, v, h, n = state.u, state.v, state.h, state.u.shape[0]\n",
    "\n",
    "        def f_tilde(u_, v_, h_):\n",
    "            return self._lyapunov(cot_lp, u_, v_, h_, sns=True)\n",
    "\n",
    "        newton_wrap = NewtonWrapper(\n",
    "            f_tilde, jax.grad(f_tilde), jax.hessian(f_tilde)\n",
    "        )\n",
    "\n",
    "        transp = self._apply_transport(cot_lp, u, v, h)\n",
    "\n",
    "        H = -(1 / cot_lp.geom.epsilon) * self._approximate_hessian(\n",
    "            f_tilde, transp, u, v, h, rho\n",
    "        )\n",
    "        H = self._regularize_hessian(H, n=u.shape[0])\n",
    "\n",
    "        z = jnp.concatenate([u, v, h])\n",
    "        delta_z, info = self._solve_linear_system(\n",
    "            H, -newton_wrap.grad(u, v, h), **kwargs\n",
    "        )\n",
    "        if info != 0:\n",
    "            print(\"CG did not converge.\")\n",
    "\n",
    "        alpha = self._backtracking_line_search(newton_wrap, z, delta_z)\n",
    "        z += alpha * delta_z\n",
    "\n",
    "        return ConstrainedSinkhornState(u=z[:n], v=z[n : 2 * n], h=z[2 * n :])\n",
    "\n",
    "    def __call__(self, cot_lp) -> ConstrainedSinkhornState:\n",
    "        \"\"\"Run the constrained Sinkhorn solver.\n",
    "        Args:\n",
    "            cot_lp: Constrained linear problem.\n",
    "        Returns:\n",
    "            ConstrainedSinkhornState: Final state of the solver.\n",
    "        \"\"\"\n",
    "        state = self.init(cot_lp)\n",
    "\n",
    "        def body_fn(state, iteration):\n",
    "            if iteration < self.N1:\n",
    "                new_state = self._one_iteration_sinkhorn(\n",
    "                    cot_lp, state, iteration\n",
    "                )\n",
    "            elif iteration < self.N1 + self.N2 and self.use_sns:\n",
    "                new_state = self._one_iteration_newton(cot_lp, state, iteration)\n",
    "            else:\n",
    "                new_state = state  # No more updates after N1 + N2\n",
    "            return new_state, None\n",
    "\n",
    "        def cond_fn(val):\n",
    "            state, iteration = val\n",
    "            return (iteration < self.max_iterations) & (\n",
    "                (iteration < self.min_iterations)\n",
    "                | (state.error is None)\n",
    "                | (state.error > self.threshold)\n",
    "            )\n",
    "\n",
    "        iteration = 0\n",
    "        val = (state, iteration)\n",
    "\n",
    "        while cond_fn(val):\n",
    "            state, _ = body_fn(state, iteration)\n",
    "            iteration += 1\n",
    "            val = (state, iteration)\n",
    "\n",
    "        converged = iteration < self.max_iterations\n",
    "        return ConstrainedSinkhornState(\n",
    "            u=state.u, v=state.v, h=state.h, converged=converged\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562730ae",
   "metadata": {},
   "source": [
    "### Difficulties\n",
    "\n",
    "I wanted to implement the second algorithm of {cite}`tang:24`, I encountered some difficulties as jax seems not to be fully compatible with sparse matrix representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bcd34c",
   "metadata": {},
   "source": [
    "## Numerical Experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "constrained_ot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
